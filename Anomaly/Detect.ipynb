{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZJ2hRICOjzmQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('modified_UCI.csv')"
      ],
      "metadata": {
        "id": "SZSPcRDHZsXW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head"
      ],
      "metadata": {
        "id": "occzKQ3-UAdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the result\n",
        "print(\"Total missing values in each column:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "n7Na5Nvra3uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "1a2qyOQiUer9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "\n",
        "df = pd.read_csv('modified_UCI.csv')\n",
        "\n",
        "# Extract relevant columns (date, time, AH)\n",
        "time_series_data = df[['Date', 'Time', 'AH']]\n",
        "\n",
        "\n",
        "time_series_data['datetime'] = pd.to_datetime(time_series_data['Date'] + ' ' + time_series_data['Time'])\n",
        "time_series_data.set_index('datetime', inplace=True)\n",
        "\n",
        "# Drop the individual date and time columns\n",
        "time_series_data.drop(['Date', 'Time'], axis=1, inplace=True)\n",
        "\n",
        "# Normalize values to 0,1\n",
        "scaler = MinMaxScaler()\n",
        "time_series_data['normalized'] = scaler.fit_transform(time_series_data[['AH']])\n",
        "\n",
        "# Function to create sequences for time-series data\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences, targets = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        seq = data.iloc[i:i + sequence_length]\n",
        "        label = data.iloc[i + sequence_length]['normalized']\n",
        "        sequences.append(seq.values)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# create sequences and define length\n",
        "sequence_length = 10\n",
        "sequences, targets = create_sequences(time_series_data, sequence_length)\n",
        "\n",
        "\n",
        "split = int(0.8 * len(sequences))\n",
        "X_train, y_train = sequences[:split], targets[:split]\n",
        "X_test, y_test = sequences[split:], targets[split:]\n",
        "\n",
        "# RNN model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "#  Mean Squared Error\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "\n"
      ],
      "metadata": {
        "id": "O2zSF4PFGBp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autoencoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "\n",
        "# Select features (X)\n",
        "X = df[['C6H6(GT)', 'CO(GT)', 'NOx(GT)', 'NO2(GT)', 'NOx(GT)']]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=32)\n",
        "\n",
        "# autoencoder model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=X_train.shape[1], activation='linear'))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the autoencoder\n",
        "model.fit(X_train, X_train, epochs=20, batch_size=22, validation_split=0.2, verbose=1)\n",
        "\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate the reconstruction error on the test set\n",
        "test_mse = mean_squared_error(X_test, test_predictions)\n",
        "print(f'Test Mean Squared Error: {test_mse}')\n",
        "\n",
        "\n",
        "\n",
        "threshold = 0.000003\n",
        "\n",
        "\n",
        "anomalies = test_mse > threshold\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cjKpN2DljwiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "features = df[['C6H6(GT)', 'CO(GT)', 'NOx(GT)', 'NO2(GT)']]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "\n",
        "threshold = 0.05\n",
        "df['anomaly'] = (features_scaled.mean(axis=1) > threshold).astype(int)\n",
        "\n",
        "\n",
        "dbscan_model = DBSCAN(eps=1.5, min_samples=5)\n",
        "dbscan_labels = dbscan_model.fit_predict(features_scaled)\n",
        "\n",
        "\n",
        "df['dbscan_label'] = dbscan_labels\n",
        "df['is_anomaly_dbscan'] = (df['dbscan_label'] == -1).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(df['anomaly'], df['is_anomaly_dbscan']))\n",
        "\n",
        "# indices of anomalies detected by DBSCAN\n",
        "anomaly_indices_dbscan = df.index[df['is_anomaly_dbscan'] == 1]\n",
        "print(\"Anomaly Indices (DBSCAN):\", anomaly_indices_dbscan)\n"
      ],
      "metadata": {
        "id": "Pffg5ga8rcx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "features = df[['C6H6(GT)', 'CO(GT)', 'NOx(GT)', 'NO2(GT)']]\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "\n",
        "threshold = 0.03\n",
        "df['anomaly'] = (features_scaled.mean(axis=1) > threshold).astype(int)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, df['anomaly'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the k-NN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "anomaly_indices = df.index[df['anomaly'] == 1]\n",
        "print(\"Anomaly Indices:\", anomaly_indices)\n"
      ],
      "metadata": {
        "id": "iNcYxPl5fXWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}